1.  Login to the machine as a non-super user

2.  Ensure, your user is in the sodoers file, so that you can execute commands as sudo

3.  Execute the following to update the system
        sudo apt update

4.  If you wish to upgrade the system, execute the following
        sudo apt upgrade -y

5.  Since Apache-Spark is a java based application, ensure you have JDK installed.
    Execute the following to check, if java is already installed on the OS
        java --version

        Output:
        devops@ubuntu:/$ java --version
        openjdk 11.0.17 2022-10-18
        OpenJDK Runtime Environment (build 11.0.17+8-post-Ubuntu-1ubuntu222.04)
        OpenJDK 64-Bit Server VM (build 11.0.17+8-post-Ubuntu-1ubuntu222.04, mixed mode, sharing)
        devops@ubuntu:/$

6.  If Java is not installed, execute the following to install it on Ubuntu
        sudo apt install default-jdk

7.  The current working directory is 'your home' directory on the Server
        /home/devops/
    Since you logged in, you have not changed in to another directory.

8.  Execute the following to download the tarball from Apache Spark's website
        wget https://archive.apache.org/dist/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz

9.  Once the tar ball is downloaded successfully, untar it, by executing the following command
        tar xvf spark-3.0.3-bin-hadoop2.7.tgz

10. Move the expanded tar ball to /opt/, by executing the following command
        sudo mv ~/spark-3.0.3-bin-hadoop2.7/ /opt/

11. Rename the 'spark-3.0.3-bin-hadoop2.7' directory to 'spark' under /opt/, by executing following command
        sudo mv /opt/spark-3.0.3-bin-hadoop2.7 /opt/spark

12. Now, its time to configure few variables for Spark
    Write the following lines to '.bashrc' or '.profile' under your home directory, and save the file

        export SPARK_HOME=/opt/spark
        export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
        export PYSPARK_PYTHON=/usr/bin/python3
    
13. For the variables to take effect, source / execute the '.profile' under your home directory
        source ~/.profile

14. The default port for Spark is 8080, however if for whatever reason, this port is occupied, then spark will try to use 8081.

15. I prefer, 'firewalld' instead of the default 'ufw' for firewall services on ubuntu. You can install firewalld, but executing the following command
        sudo apt install firewalld
    
    You can verify the install, by executing the following command
        dpkg -L firewalld
    
    You can check the version of the firewall, by executing the following command
        sudo firewall-cmd --version

16. Check the firewall service status by executing the following command
        sudo systemctl status firewalld

        Output:
            devops@ubuntu:~$ systemctl status firewalld
            ● firewalld.service - firewalld - dynamic firewall daemon
                Loaded: loaded (/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled)
                Active: active (running) since Thu 2023-01-05 17:28:04 UTC; 47min ago
                Docs: man:firewalld(1)
                Main PID: 4128 (firewalld)
                Tasks: 2 (limit: 18905)
                Memory: 27.0M
                CPU: 843ms
                CGroup: /system.slice/firewalld.service
                        └─4128 /usr/bin/python3 /usr/sbin/firewalld --nofork --nopid

17. Enable the firewall service to start with system boot up
        sudo systemctl enable firewalld

18. Since I have firewalld, I went and disabled 'ufw' via the following command
        sudo systemctl stop ufw
        sudo systemctl disable ufw

19. Now, its time to start the standalone master of Spark, by executing the following command
        start-master.sh

20. The logs for start up of master are present at the following location
        /opt/spark/logs

        If the master started successfully, it will have a line printed as the following
            Successfully started service 'MasterUI' on port 8080.
        
        You can access the web-ui for spark with the following address, but you need to ensure, firewall is not blocking 8081
            http://your-machine-ip:8080

21. To check, what all ports are allowed for traffice, execute the following command
        sudo firewall-cmd --list-all | grep -i '^  ports'

        If none of the ports are opened, the output will look like the following

        Output:
            ports: 

22. To open the port 8080, execute the following commands
        sudo firewall-cmd --permanent --add-port=8080/tcp
        sudo firewall-cmd --reload

21. Now, its time to start the slave, by executing the following command
    Their are two ways to start the slave or worker, the default command is
        start-slave.sh spark://ubuntu:7077

    The challenge with this approach is, it will use available 'cores' and 'RAM' available on the machine to do its processing
    Hence, starting a slave / worker with limited number of 'cores' and 'RAM' is a more optimal approach

        start-slave.sh -c 1 -m 256M spark://ubuntu:7077

Screenshot of Spark GUI is in the same directory, to see how it looks with a Master and Slave started        
